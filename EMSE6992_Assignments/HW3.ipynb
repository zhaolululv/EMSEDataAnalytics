{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3\n",
    "\n",
    "### Due by 11:59pm on 30 November ( submit within Portfolio )\n",
    "\n",
    "Pick 10 of the following instructions and try to implement them on your dataset and show your code and execution within the Jupyter Notebook.\n",
    "\n",
    "For HW3 we will be using multiple Python Data Science packages. Follow the instructions and try to duplicate the examples provided within the instruction.\n",
    "\n",
    "#### For each instruction, try to run the example on your dataset.  If you cannot run the example on your dataset, then use the Iris, Boston, Digits, Wine, etc. Data set.\n",
    "\n",
    "Once complete, upload your results to Github and update the Assignment 3 link within your portfolio's.\n",
    "\n",
    "### Concatenating\n",
    "Instruction 1: Provide an example of concatenating multiple feature extraction methods using your dataset.\n",
    "\n",
    "In many real-world examples, there are many ways to extract features from a dataset. Often it is beneficial to combine several methods to obtain good performance. This example shows how to use FeatureUnion to combine features obtained by PCA and univariate selection.\n",
    "Combining features using this transformer has the benefit that it allows cross validation and grid searches over the whole process.\n",
    "The combination used in this example is not particularly helpful on this dataset and is only used to illustrate the usage of FeatureUnion.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/plot_feature_stacker.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/plot_feature_stacker.ipynb\n",
    "    \n",
    "\n",
    "### Applications - Boston Housing Dataset\n",
    "Instruction 2: Provide an example of Outlier detection on your dataset.\n",
    "\n",
    "This example illustrates the need for robust covariance estimation on a real data set. It is useful both for outlier detection and for a better understanding of the data structure.\n",
    "We selected two sets of two variables from the Boston housing data set as an illustration of what kind of analysis can be done with several outlier detection tools. For the purpose of visualization, we are working with two-dimensional examples, but one should be aware that things are not so trivial in high-dimension, as it will be pointed out.\n",
    "In both examples below, the main result is that the empirical covariance estimate, as a non-robust one, is highly influenced by the heterogeneous structure of the observations. Although the robust covariance estimate is able to focus on the main mode of the data distribution, it sticks to the assumption that the data should be Gaussian distributed, yielding some biased estimation of the data structure, but yet accurate to some extent. The One-Class SVM does not assume any parametric form of the data distribution and can therefore model the complex shape of the data much better.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/applications/plot_outlier_detection_housing.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/applications/plot_outlier_detection_housing.ipynb\n",
    "\n",
    "### Classification Comparison and Probability\n",
    "Instruction 3: Provide an example of Classifier Comparison using your dataset\n",
    "\n",
    "A comparison of a several classifiers in scikit-learn on synthetic datasets. The point of this example is to illustrate the nature of decision boundaries of different classifiers. This should be taken with a grain of salt, as the intuition conveyed by these examples does not necessarily carry over to real datasets.\n",
    "Particularly in high-dimensional spaces, data can more easily be separated linearly and the simplicity of classifiers such as naive Bayes and linear SVMs might lead to better generalization than is achieved by other classifiers.\n",
    "The plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set.\n",
    "\n",
    "Plot the classification probability for different classifiers. We use a 3 class dataset, and we classify it with a Support Vector classifier, L1 and L2 penalized logistic regression with either a One-Vs-Rest or multinomial setting, and Gaussian process classification.\n",
    "The logistic regression is not a multiclass classifier out of the box. As a result it can identify only the first class.\n",
    "\n",
    "Example file located in:\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/classification/plot_lda_qda.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/Classification/plot_classifier_comparison.ipynb\n",
    "\n",
    "### Clustering\n",
    "Instruction 4: Provide an example of K-means Clustering using your dataset\n",
    "\n",
    "The plots display firstly what a K-means algorithm would yield using three clusters. It is then shown what the effect of a bad initialization is on the classification process: By setting n_init to only 1 (default is 10), the amount of times that the algorithm will be run with different centroid seeds is reduced. The next plot displays what using eight clusters would deliver and finally the ground truth.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/clustering/plot_cluster_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/cluster/plot_cluster_iris.ipynb\n",
    "\n",
    "\n",
    "### Covariance Estimation\n",
    "Instruction 4: Provide an example of Outlier detection with covariance estimation using your dataset.\n",
    "\n",
    "When the amount of contamination is known, this example illustrates three different ways of performing Novelty and Outlier Detection:\n",
    "•\tbased on a robust estimator of covariance, which is assuming that the data are Gaussian distributed and performs better than the One-Class SVM in that case.\n",
    "•\tusing the One-Class SVM and its ability to capture the shape of the data set, hence performing better when the data is strongly non-Gaussian, i.e. with two well-separated clusters;\n",
    "•\tusing the Isolation Forest algorithm, which is based on random forests and hence more adapted to large-dimensional settings, even if it performs quite well in the examples below.\n",
    "•\tusing the Local Outlier Factor to measure the local deviation of a given data point with respect to its neighbors by comparing their local density.\n",
    "The ground truth about inliers and outliers is given by the points colors while the orange-filled area indicates which points are reported as inliers by each method.\n",
    "Here, we assume that we know the fraction of outliers in the datasets.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/Covariance/plot_outlier_detection.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/covariance/plot_outlier_detection.ipynb\n",
    "\n",
    "### Cross decomposition\n",
    "Instruction 5: Provide a comparison of cross decomposition methods using your dataset\n",
    "\n",
    "Simple usage of various cross decomposition algorithms: - PLSCanonical - PLSRegression, with multivariate response, a.k.a. PLS2 - PLSRegression, with univariate response, a.k.a. PLS1 - CCA\n",
    "Given 2 multivariate covarying two-dimensional datasets, X, and Y, PLS extracts the ‘directions of covariance’, i.e. the components of each datasets that explain the most shared variance between both datasets. This is apparent on the scatterplot matrix display: components 1 in dataset X and dataset Y are maximally correlated (points lie around the first diagonal). This is also true for components 2 in both dataset, however, the correlation across datasets for different components is weak: the point cloud is very spherical.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/cross_decomposition/plot_compare_cross_decomposition.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/cross_decomposition/plot_compare_cross_decomposition.ipynb\n",
    "\n",
    "### Decomposition - Iris Dataset\n",
    "Instruction 6: Provide an example of PCA using your dataset\n",
    "\n",
    "Principal Component Analysis applied to the Iris dataset.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/decomposition/plot_pca_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/decomposition/plot_pca_iris.ipynb\n",
    "\n",
    "### LDA and PCA - Iris Dataset\n",
    "Instruction 7: Provide an example of a Comparison of LDA and PCA 2D projection of your dataset\n",
    "\n",
    "The Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour and Virginica) with 4 attributes: sepal length, sepal width, petal length and petal width.\n",
    "Principal Component Analysis (PCA) applied to this data identifies the combination of attributes (principal components, or directions in the feature space) that account for the most variance in the data. Here we plot the different samples on the 2 first principal components.\n",
    "Linear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance between classes. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/decomposition/plot_pca_vs_lda.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/decomposition/plot_pca_vs_lda.ipynb\n",
    "\n",
    "### Ensemble methods - Iris Dataset\n",
    "Instruction 8: Provide an example of Plotting the decision surfaces of ensembles of trees using your dataset\n",
    "\n",
    "Plot the decision surfaces of forests of randomized trees trained on pairs of features of the iris dataset.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/ensemble/plot_forest_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/ensemble/plot_forest_iris.ipynb\n",
    "\n",
    "### Support Vector Machine - Part 1 - Iris Dataset\n",
    "Instruction 9: Provide an example of SVM using your dataset\n",
    "\n",
    "A tutorial exercise for using different SVM kernels.\n",
    "This exercise is used in the Using kernels part of the Supervised learning: predicting an output variable from high-dimensional observations section of the A tutorial on statistical-learning for scientific data processing.\n",
    "\n",
    " Dataset Exercise\n",
    "A tutorial exercise using Cross-validation with an SVM on the Digits dataset.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/exercises/plot_cv_digits.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/exercises/plot_iris_exercise.ipynb\n",
    "\n",
    "### Feature Selection - Boston Dataset\n",
    "Instruction 10: Provide an example of Feature selection using SelectFromModel and LassoCV using your dataset\n",
    "\n",
    "Use SelectFromModel meta-transformer along with Lasso to select the best couple of features from the Boston dataset.\n",
    "plot_select_from_model_boston.ipynb\n",
    "\n",
    "Example file located in: auto_examples_jupyter/feature_selection/plot_select_from_model_boston.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/feature_selection/plot_select_from_model_boston.ipynb\n",
    "\n",
    "### Univariate Feature Selection - Iris Dataset\n",
    "Instruction 11: Provide an example of Univariate Feature Selection using your dataset.\n",
    "\n",
    "An example showing univariate feature selection.\n",
    "Noisy (non informative) features are added to the iris data and univariate feature selection is applied. For each feature, we plot the p-values for the univariate feature selection and the corresponding weights of an SVM. We can see that univariate feature selection selects the informative features and that these have larger SVM weights.\n",
    "In the total set of features, only the 4 first ones are significant. We can see that they have the highest score with univariate feature selection. The SVM assigns a large weight to one of these features, but also Selects many of the non-informative features. Applying univariate feature selection before the SVM increases the SVM weight attributed to the significant features, and will thus improve classification\n",
    "\n",
    "Example file located in: auto_examples_jupyter/feature_selection/plot_feature_selection.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/feature_selection/plot_feature_selection.ipynb\n",
    "\n",
    "\n",
    "### Gaussian Process for Machine Learning - Iris Dataset\n",
    "Instruction 12: Provide an example of Gaussian process classification (GPC) on your dataset\n",
    "\n",
    "This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions.\n",
    "\n",
    "Example file located in:\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/gaussian_process/plot_gpc_iris.ipynb\n",
    "\n",
    "### Generalized Linear Models - Iris Dataset\n",
    "Instruction 13: Provide an example of Plotting multi-class SGD on your dataset\n",
    "\n",
    "Plot decision surface of multi-class SGD on iris dataset. The hyperplanes corresponding to the three one-versus-all (OVA) classifiers are represented by the dashed lines.\n",
    "plot_sgd_iris.ipynb\n",
    "Logistic Regression 3-class Classifier\n",
    "Show below is a logistic-regression classifiers decision boundaries on the iris <https://en.wikipedia.org/wiki/Iris_flower_data_set>_ dataset. The datapoints are colored according to their labels.\n",
    "\n",
    "Example file located in:\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/linear_model/plot_iris_logistic.ipynb\n",
    "\n",
    "### Model Selection\n",
    "Instruction 14: Provide an example of Underfitting vs. Overfitting using your dataset\n",
    "\n",
    "This example demonstrates the problems of underfitting and overfitting and how we can use linear regression with polynomial features to approximate nonlinear functions. The plot shows the function that we want to approximate, which is a part of the cosine function. In addition, the samples from the real function and the approximations of different models are displayed. The models have polynomial features of different degrees. We can see that a linear function (polynomial with degree 1) is not sufficient to fit the training samples. This is called underfitting. A polynomial of degree 4 approximates the true function almost perfectly. However, for higher degrees the model will overfit the training data, i.e. it learns the noise of the training data. We evaluate quantitatively overfitting / underfitting by using cross-validation. We calculate the mean squared error (MSE) on the validation set, the higher, the less likely the model generalizes correctly from the training data.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/model_selection/plot_underfitting_overfitting.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/model_selection/plot_underfitting_overfitting.ipynb\n",
    "\n",
    "### Nearest Neighbors\n",
    "Instruction 15: Provide an example of Nearest Neighbors Classification using your dataset.\n",
    "\n",
    "Sample usage of Nearest Neighbors classification. It will plot the decision boundaries for each class.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/neighbors/plot_classification.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/neighbors/plot_classification.ipynb\n",
    "\n",
    "### Neural Networks\n",
    "Instruction 16: Provide an example of Varying regularization in Multi-layer Perceptron using your dataset\n",
    "\n",
    "A comparison of different values for regularization parameter ‘alpha’ on synthetic datasets. The plot shows that different alphas yield different decision functions.\n",
    "Alpha is a parameter for regularization term, aka penalty term, that combats overfitting by constraining the size of the weights. Increasing alpha may fix high variance (a sign of overfitting) by encouraging smaller weights, resulting in a decision boundary plot that appears with lesser curvatures. Similarly, decreasing alpha may fix high bias (a sign of underfitting) by encouraging larger weights, potentially resulting in a more complicated decision boundary.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/neural_networks/plot_mlp_alpha.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/neural_networks/plot_mlp_alpha.ipynb\n",
    "\n",
    "### Preprocessing - Wine Dataset\n",
    "\n",
    "Instruction 17: Provide an example of Importance of Feature Scaling using your dataset\n",
    "\n",
    "Feature scaling though standardization (or Z-score normalization) can be an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.\n",
    "While many algorithms (such as SVM, K-nearest neighbors, and logistic regression) require features to be normalized, intuitively we can think of Principle Component Analysis (PCA) as being a prime example of when normalization is important. In PCA we are interested in the components that maximize the variance. If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the ‘weight’ axis, if those features are not scaled. As a change in height of one meter can be considered much more important than the change in weight of one kilogram, this is clearly incorrect.\n",
    "To illustrate this, PCA is performed comparing the use of data with StandardScaler applied, to unscaled data. The results are visualized and a clear difference noted. The 1st principal component in the unscaled set can be seen. It can be seen that feature #13 dominates the direction, being a whole two orders of magnitude above the other features. This is contrasted when observing the principal component for the scaled version of the data. In the scaled version, the orders of magnitude are roughly the same across all the features.\n",
    "The dataset used is the Wine Dataset available at UCI. This dataset has continuous features that are heterogeneous in scale due to differing properties that they measure (i.e alcohol content, and malic acid).\n",
    "The transformed data is then used to train a naive Bayes classifier, and a clear difference in prediction accuracies is observed wherein the dataset which is scaled before PCA vastly outperforms the unscaled version.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/preprocessing/plot_scaling_importance.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/preprocessing/plot_scaling_importance.ipynb\n",
    "\n",
    "### Semi Supervised Classification - Iris Dataset\n",
    "Instruction 19: Provide an example of the decision boundary of label propagation versus SVM on your dataset\n",
    "\n",
    "Comparison for decision boundary generated on iris dataset between Label Propagation and SVM.\n",
    "This demonstrates Label Propagation learning a good boundary even with a small amount of labeled data.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/semi_supervised/plot_label_propagation_versus_svm_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/semi_supervised/plot_label_propagation_versus_svm_iris.ipynb\n",
    "\n",
    "### Support Vector Machines - Part 2 - Iris Dataset\n",
    "Instruction 20: Provide an example of Plot different SVM classifiers in your dataset\n",
    "Comparison of different linear SVM classifiers on a 2D projection of the iris dataset. We only consider the first 2 features of this dataset:\n",
    "•\tSepal length\n",
    "•\tSepal width\n",
    "This example shows how to plot the decision surface for four SVM classifiers with different kernels.\n",
    "The linear models LinearSVC() and SVC(kernel='linear') yield slightly different decision boundaries. This can be a consequence of the following differences:\n",
    "•\tLinearSVC minimizes the squared hinge loss while SVC minimizes the regular hinge loss.\n",
    "•\tLinearSVC uses the One-vs-All (also known as One-vs-Rest) multiclass reduction while SVC uses the One-vs-One multiclass reduction.\n",
    "Both linear models have linear decision boundaries (intersecting hyperplanes) while the non-linear kernel models (polynomial or Gaussian RBF) have more flexible non-linear decision boundaries with shapes that depend on the kind of kernel and its parameters.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/svm/plot_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/svm/plot_iris.ipynb\n",
    "\n",
    "### Decision Trees - Iris Dataset\n",
    "Instruction 21: Provide an example of plotting the decision surface of a decision tree on your dataset\n",
    "\n",
    "Plot the decision surface of a decision tree trained on pairs of features of the iris dataset.\n",
    "See decision tree for more information on the estimator.\n",
    "For each pair of iris features, the decision tree learns decision boundaries made of combinations of simple thresholding rules inferred from the training samples.\n",
    "\n",
    "Example file located in: auto_examples_jupyter/tree/plot_iris.ipynb\n",
    " - https://github.com/bsharvey/EMSEDataAnalytics/blob/master/auto_examples_jupyter/tree/plot_iris.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
